# OpenVRQoE

## 1. Visual Attention
### (1) 360◦ Image
<table>
<tbody>
    <tr>
        <th>Index</th><th>Name</th><th>URL</th><th>Year</th><th>Affiliation</th>
    <tr>
        <td>[1]</td>
        <td>SUN360</td>
        <td>http://3dvision.princeton.edu/projects/2012/SUN360/</td>
        <td>2012</td>
        <td>Princeton University</td>
    </tr>
    <tr>
        <td>[2]</td>
        <td>Salient!360</td>
        <td>https://www.technicolor.com/dream/researchinnovation/salient-dataset-download</td>
        <td>2017</td>
        <td>University of Nantes</td>
    </tr>
    <tr>
        <td>[3]</td>
        <td>ODI Saliency</td>
        <td>https://vsense.scss.tcd.ie/research/3dof/testbed/</td>
        <td>2017</td>
        <td>Trinity College Dublin</td>
    </tr>
    <tr>
        <td>[4]</td>
        <td>VR Saliency</td>
        <td>https://vsitzmann.github.io/vr-saliency/</td>
        <td>2018</td>
        <td>Trinity College Dublin</td>
    </tr>
    <tr>
        <td>[5]</td>
        <td>F-360iSOD</td>
        <td>https://github.com/PanoAsh/F-360iSOD</td>
        <td>2020</td>
        <td>University of Rennes</td>
    </tr>
</table>
          
### (2) 360◦ Video
<table>
<tbody>
    <tr>
        <th>Index</th><th>Name</th><th>URL</th><th>Year</th><th>Affiliation</th>
    <tr>
        <td>[1]</td>
        <td>Moving Target</td>
        <td>http://360videoexp.com/</td>
        <td>2016</td>
        <td>University of California</td>
    </tr>
    <tr>
        <td>[2]</td>
        <td>360 Video Database</td>
        <td>http://vhil.stanford.edu/360-video-database/</td>
        <td>2017</td>
        <td>Stanford University</td>
    </tr>
    <tr>
        <td>[3]</td>
        <td>360 Head Movement</td>
        <td>http://dash.ipv6.enstb.fr/headMovements/</td>
        <td>2017</td>
        <td>IMT Atlantique</td>
    </tr>
    <tr>
        <td>[4]</td>
        <td>VR User Behavior</td>
        <td>https://wuchlei-thu.github.io/</td>
        <td>2017</td>
        <td>Tsinghua University</td>
    </tr>
    <tr>
        <td>[5]</td>
        <td>VR-HM48</td>
        <td>https://github.com/Archer-Tatsu/head-tracking</td>
        <td>2017</td>
        <td>Beihang University</td>
    </tr>
    <tr>
        <td>[6]</td>
        <td>360 Dataset</td>
        <td>https://nmsl.cs.nthu.edu.tw/360-video-project/</td>
        <td>2017</td>
        <td>National Tsing Hua University</td>
    </tr>
    <tr>
        <td>[7]</td>
        <td>Sports-360</td>
        <td>http://aliensunmin.github.io/project/360video/</td>
        <td>2017</td>
        <td>National Tsing Hua University</td>
    </tr>
    <tr>
        <td>[8]</td>
        <td>UN Salient360</td>
        <td>https://salient360.ls2n.fr/</td>
        <td>2018</td>
        <td>University of Nantes</td>
    </tr>
    <tr>
        <td>[9]</td>
        <td>PanoSaliency</td>
        <td>https://github.com/phananh1010/PanoSaliency</td>
        <td>2019</td>
        <td>Georgia State University</td>
    </tr>
    <tr>
        <td>[10]</td>
        <td>PVS-HM</td>
        <td>https://github.com/YuhangSong/DHP</td>
        <td>2019</td>
        <td>Beihang University</td>
    </tr>
    <tr>
        <td>[11]</td>
        <td>VR Eye-Tracking</td>
        <td>https://github.com/xuyanyu-shh/VR-EyeTracking</td>
        <td>2018</td>
        <td>ShanghaiTech University</td>
    </tr>
    <tr>
        <td>[12]</td>
        <td>Desktop360</td>
        <td>http://videolab.engineering.nyu.edu/Desktop360/</td>
        <td>2018</td>
        <td>New York University</td>
    </tr>
    <tr>
        <td>[13]</td>
        <td>AVTrack360</td>
        <td>https://github.com/acmmmsys/2018-AVTrack360</td>
        <td>2018</td>
        <td>Technical University Ilmenau</td>
    </tr>
    <tr>
        <td>[14]</td>
        <td>360 EM Dataset</td>
        <td>https://web.gin.gnode.org/ioannis.agtzidis/360_em_dataset</td>
        <td>2019</td>
        <td>Technical University of Munich</td>
    </tr>
</table>

## 2. QoE
### 2.1 Data
#### (1) 360◦ Image
<table>
<tbody>
    <tr>
        <th>Index</th><th>Name</th><th>URL</th><th>Year</th><th>Affiliation</th>
    <tr>
        <td>[1]</td>
        <td>Upenik</td>
        <td>-</td>
        <td>2016</td>
        <td>Ecole Polytechnique Federale de Lausanne</td>
    </tr>
    <tr>
        <td>[2]</td>
        <td>Upenik</td>
        <td>-</td>
        <td>2017</td>
        <td>Ecole Polytechnique Federale de Lausanne</td>
    </tr>
    <tr>
        <td>[3]</td>
        <td>CVIQD</td>
        <td>-</td>
        <td>2017</td>
        <td>Shanghai Jiao Tong University</td>
    </tr>
    <tr>
        <td>[4]</td>
        <td>CVIQD2018</td>
        <td>https://github.com/sunwei925/CVIQDatabase</td>
        <td>2018</td>
        <td>Shanghai Jiao Tong University</td>
    </tr>
    <tr>
        <td>[5]</td>
        <td>Huang</td>
        <td>https://live.ece.utexas.edu/research/quality/immersive_images/</td>
        <td>2018</td>
        <td>Nanjing University</td>
    </tr>
    <tr>
        <td>[6]</td>
        <td>OIQA</td>
        <td>https://duanhuiyu.github.io/</td>
        <td>2018</td>
        <td>Shanghai Jiao Tong University</td>
    </tr>
    <tr>
        <td>[7]</td>
        <td>Upenik</td>
        <td>https://github.com/mmspg/saliencymetric360</td>
        <td>2019</td>
        <td>Ecole Polytechnique Federale de Lausanne</td>
    </tr>
    <tr>
        <td>[8]</td>
        <td>Kim</td>
        <td>http://ivylabdb.kaist.ac.kr</td>
        <td>2020</td>
        <td>Korea Advanced Institute of Science and Technology</td>
    </tr>
    <tr>
        <td>[9]</td>
        <td>Jabar</td>
        <td>https://github.com/jascenso/IST-GPP-Dataset</td>
        <td>2020</td>
        <td>Universidade de Lisboa-Instituto de Telecomunicacoes</td>
    </tr>
    <tr>
        <td>[10]</td>
        <td>ISIQA</td>
        <td>https://pavancm.github.io/stitched-qa/</td>
        <td>2021</td>
        <td>Indian Institute of Science</td>
    </tr>
    <tr>
        <td>[11]</td>
        <td>MVAQD</td>
        <td>https://github.com/Jianghao2019/MVAQD</td>
        <td>2021</td>
        <td>Ningbo University</td>
    </tr>
    <tr>
        <td>[12]</td>
        <td>IQA-ODI</td>
        <td>https://github.com/yanglixiaoshen/SAP-Net</td>
        <td>2021</td>
        <td>Beihang University</td>
    </tr>
    <tr>
        <td>[13]</td>
        <td>Fang</td>
        <td>https://github.com/LXLHXL123/JUFE-VRIQA</td>
        <td>2022</td>
        <td>Jiangxi University of Finance and Economics</td>
    </tr>
    <tr>
        <td>[14]</td>
        <td>OSIQA</td>
        <td>https://duanhuiyu.github.io/</td>
        <td>2023</td>
        <td>Shanghai Jiao Tong University</td>
    </tr>
    <tr>
        <td>[15]</td>
        <td>Simka</td>
        <td>https://zenodo.org/records/10422786</td>
        <td>2024</td>
        <td>Brno University of Technology</td>
    </tr>
    <tr>
        <td>[16]</td>
        <td>JUFE-10K</td>
        <td>https://github.com/RJL2000/OIQAND</td>
        <td>2025</td>
        <td>Jiangxi University of Finance and Economics</td>
    </tr>
    <tr>
        <td>[17]</td>
        <td>OIQ-10K</td>
        <td>https://github.com/WenJuing/IQCaption360</td>
        <td>2025</td>
        <td>Jiangxi University of Finance and Economics</td>
    </tr>
    <tr>
        <td>[18]</td>
        <td>SOLID</td>
        <td>https://faculty.ustc.edu.cn/chenzhibo/zh_CN/article/988216/content/2461.htm#article</td>
        <td>2018</td>
        <td>University of Science and Technology of China</td>
    </tr>
    <tr>
        <td>[19]</td>
        <td>LIVE 3D VR IQA</td>
        <td>http://live.ece.utexas.edu/research/VR3D/index.html</td>
        <td>2020</td>
        <td>University of Texas at Austin</td>
    </tr>
    <tr>
        <td>[20]</td>
        <td>NBU-SOID</td>
        <td>https://github.com/qyb123/NBU-SOID</td>
        <td>2020</td>
        <td>Ningbo University</td>
    </tr>
</table>

#### (2) 360◦ Video
<table>
<tbody>
    <tr>
        <th>Index</th><th>Name</th><th>URL</th><th>Year</th><th>Affiliation</th>
    <tr>
        <td>[1]</td>
        <td>IVQAD</td>
        <td>https://duanhuiyu.github.io/</td>
        <td>2017</td>
        <td>Shanghai Jiao Tong University</td>
    </tr>
    <tr>
        <td>[2]</td>
        <td>ODV Dataset</td>
        <td>https://v-sense.scss.tcd.ie/research/voronoibased-objective-metrics/</td>
        <td>2019</td>
        <td>Trinity College Dublin</td>
    </tr>
    <tr>
        <td>[3]</td>
        <td>VRQ-TJU</td>
        <td>ftp://eeec.tju.edu.cn/VR</td>
        <td>2018</td>
        <td>Tianjin University</td>
    </tr>
    <tr>
        <td>[4]</td>
        <td>VR-VQA48</td>
        <td>https://github.com/Archer-Tatsu/head-tracking</td>
        <td>2017</td>
        <td>Beihang University</td>
    </tr>
    <tr>
        <td>[5]</td>
        <td>VQA-ODV</td>
        <td>https://github.com/Archer-Tatsu/VQA-ODV</td>
        <td>2018</td>
        <td>Beihang University</td>
    </tr>
    <tr>
        <td>[6]</td>
        <td>Zhang-TBC2018</td>
        <td>-</td>
        <td>2018</td>
        <td>Wuhan University</td>
    </tr>
    <tr>
        <td>[7]</td>
        <td>Mahmoudpour-TCSVT2021</td>
        <td>http://data.etrovub.be/qualitydb/judder-vqa/</td>
        <td>2021</td>
        <td>Vrije Universiteit Brussel</td>
    </tr>
    <tr>
        <td>[8]</td>
        <td>Fang-VRW2023</td>
        <td>https://github.com/Yao-Yiru/VR-Video-Database</td>
        <td>2023</td>
        <td>Jiangxi University of Finance and Economics</td>
    </tr>
    <tr>
        <td>[9]</td>
        <td>Zou-Access2018</td>
        <td>-</td>
        <td>2018</td>
        <td>Xidian University</td>
    </tr>
    <tr>
        <td>[10]</td>
        <td>Zhang-ICASSP2019</td>
        <td>-</td>
        <td>2019</td>
        <td>Xidian University</td>
    </tr>
    <tr>
        <td>[11]</td>
        <td>Zou-ICMEW2020</td>
        <td>-</td>
        <td>2020</td>
        <td>Xidian University</td>
    </tr>
    <tr>
        <td>[12]</td>
        <td>Tran-ICUFN2017</td>
        <td>-</td>
        <td>2017</td>
        <td>The University of Aizu</td>
    </tr>
    <tr>
        <td>[13]</td>
        <td>Tran-MMSP2017</td>
        <td>-</td>
        <td>2017</td>
        <td>The University of Aizu</td>
    </tr>
    <tr>
        <td>[14]</td>
        <td>Singla-ACMM2017</td>
        <td>-</td>
        <td>2017</td>
        <td>Technical University of Ilmenau, Germany</td>
    </tr>
    <tr>
        <td>[15]</td>
        <td>Singla-QoMEX2017</td>
        <td>https://zenodo.org/record/571065#.WQmaTYh95hE</td>
        <td>2017</td>
        <td>Technical University of Ilmenau, Germany</td>
    </tr>
    <tr>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>[16]</td>
        <td>Singla-MMSys2019</td>
        <td>-</td>
        <td>2019</td>
        <td>Technical University of Ilmenau, Germany</td>
    </tr>
     <tr>
        <td>[17]</td>
        <td>Orduna-VRW2020</td>
        <td>www.gti.ssr.upm.es/data/</td>
        <td>2020</td>
        <td>Universidad Politecnica de Madrid, Spain</td>
    </tr>
     <tr>
        <td>[18]</td>
        <td>Cortes-QoMEX2020</td>
        <td>-</td>
        <td>2020</td>
        <td>Universidad Politecnica de Madrid, Spain</td>
    </tr>
     <tr>
        <td>[19]</td>
        <td>Lopes-SPIE2018</td>
        <td>-</td>
        <td>2018</td>
        <td>Universidade de Lisboa, Portugal</td>
    </tr>
     <tr>
        <td>[20]</td>
        <td>Jabar-ICMEW2020</td>
        <td></td>
        <td>2020</td>
        <td>Universidade de Lisboa, Portugal</td>
    </tr>
     <tr>
        <td>[21]</td>
        <td>Zhang-ICMEW2017</td>
        <td>-</td>
        <td>2017</td>
        <td>Beijing Institute of Technology</td>
    </tr>
     <tr>
        <td>[22]</td>
        <td>Anwar-Access2020-1</td>
        <td>-</td>
        <td>2020</td>
        <td>Beijing Institute of Technology</td>
    </tr>
     <tr>
        <td>[23]</td>
        <td> Anwar-Access2020-2</td>
        <td>-</td>
        <td>2020</td>
        <td>Beijing Institute of Technology</td>
    </tr>
     <tr>
        <td>[24]</td>
        <td>Anwar-SCIS2020</td>
        <td>-</td>
        <td>2020</td>
        <td>Beijing Institute of Technology</td>
    </tr>
     <tr>
        <td>[25]</td>
        <td>Zheleva-QoMEX2020</td>
        <td>https://osf.io/48h26/?view_only=95dcbe79c0194487a684464e7f60e63f</td>
        <td>2020</td>
        <td>Ghent University, Belgium</td>
    </tr>
     <tr>
        <td>[26]</td>
        <td>Schatz-QoMEX2017</td>
        <td>-</td>
        <td>2017</td>
        <td>Austrian Institute of Technology</td>
    </tr>
     <tr>
        <td>[27]</td>
        <td>Schatz-QoMEX2019</td>
        <td></td>
        <td>2019</td>
        <td>Austrian Institute of Technology</td>
    </tr>
     <tr>
        <td>[28]</td>
        <td>Yao-QoMEX2019</td>
        <td></td>
        <td>2019</td>
        <td>National Tsing Hua University</td>
    </tr>
     <tr>
        <td>[29]</td>
        <td>Li-GLOBECOM2018</td>
        <td>-</td>
        <td>2018</td>
        <td>Hefei University of Technology</td>
    </tr>
     <tr>
        <td>[30]</td>
        <td>Curcio-AltMM2017</td>
        <td>-</td>
        <td>2017</td>
        <td>Tampere University of Technology, Finland</td>
    </tr>
</table>

### 2.2 Model
#### (1) VRIQA
**[1] Spatial attention-based non-reference perceptual quality prediction network for omnidirectional images.**<br>
<https://github.com/yanglixiaoshen/SAP-Net><br>
**[2] MC360IQA: A multi-channel cnn for blind 360-degree image quality assessment.**<br>
<https://github.com/sunwei925/MC360IQA><br>
**[3] Blind omnidirectional image quality assessment with viewport oriented graph convolutional networks.**<br>
<https://github.com/weizhou-geek/VGCN-PyTorch><br>
**[4] Perceptual quality assessment of omnidirectional images as moving camera videos.**<br>
<https://github.com/xiangjieSui/img2video><br>
**[5] No-reference quality assessment for 360-degree images by analysis of multifrequency information and local-global naturalness.**<br>
<https://github.com/weizhou-geek/MFILGN><br>
**[6] Perceptual quality assessment of omnidirectional images.**<br>
<https://github.com/LXLHXL123/JUFE-VRIQA><br>
**[7] ST360IQ: No-reference omnidirectional image quality assessment with spherical vision transformers.**<br>
<https://github.com/Nafiseh-Tofighi/ST360IQ><br>
**[8] Assessor360: Multi-sequence network for blind omnidirectional image quality assessment.**<br>
<https://github.com/TianheWu/Assessor360><br>
**[9] Perceptual quality assessment of 360◦ images based on generative scanpath representation.**<br>
<https://github.com/xiangjieSui/GSR><br>
**[10] Multitask auxiliary network for perceptual quality assessment of non-uniformly distorted omnidirectional images.**<br>
<https://github.com/RJL2000/MTAOIQA><br>
**[11] Subjective and objective quality assessment of non-uniformly distorted omnidirectional images.**<br>
<https://github.com/RJL2000/OIQAND><br>
**[12] Omnidirectional image quality captioning: A large-scale database and a new model.**<br>
<https://github.com/WenJuing/IQCaption360><br>
**[13] Max360IQ: Blind omnidirectional image quality assessment with multi-axis attention.**<br>
<https://github.com/WenJuing/Max360IQ><br>
**[14] Viewport-unaware blind omnidirectional image quality assessment: A flexible and effective paradigm.**<br>
<https://github.com/KangchengWu/OIQA><br>

#### (2) VRVQA
**[1] Quality metric for spherical panoramic video.**<br>
<https://github.com/Samsung/360tools><br>
**[2] Weighted-to-spherically-uniform quality evaluation for omnidirectional video.**<br>
<https://github.com/Rouen007/WS-PSNR><br>
**[3] Blind VQA on 360◦ video via progressively learning from pixels, frames, and video.**<br>
<https://github.com/yanglixiaoshen/ProVQA><br>
**[4] Omnidirectional video quality assessment with causal intervention.**<br>
<https://github.com/Aca4peop/CIQNet><br>
**[5] A framework to evaluate omnidirectional video coding schemes.**<br>
<https://github.com/Samsung/360tools><br>
**[6] Quality assessment for omnidirectional video: A spatio-temporal distortion modeling approach.**<br>
<https://github.com/I2-Multimedia-Lab/360-video-experimental-platform><br>
**[7] Vewport proposal CNN for 360◦ video quality assessment.**<br>
<https://github.com/Archer-Tatsu/V-CNN><br>
**[8] Viewport-based omnidirectional video quality assessment: Database, modeling and inference.**<br>
<https://vision.nju.edu.cn/20/86/c29466a467078/page.htm><br>
**[9] Perceptual quality assessment of virtual reality videos in the wild.**<br>
<https://github.com/limuhit/VR-Video-Quality-in-the-Wild><br>
**[10] Learned scanpaths aid blind panoramic video quality assessment.**<br>
<https://github.com/kalofan/AutoScanpathQA><br>
